{
  "hash": "9bba705f839f1fad6fd6d288129cd94a",
  "result": {
    "engine": "jupyter",
    "markdown": "# Adult income dataset\nThis dataset is from [UCI machine learning repository](https://archive.ics.uci.edu/dataset/2/adult). The dataset is about the annual income of an individual and the purpose of the project is to predict whether the annual income exceeds $50K/year or not. \n\n## Reading data\n- There are two dataset files. `adult.data` is the training dataset and `adult.test` is the test dataset.\n- Both files don't have header lines. So we need to manually specify column names. \n- The first line of `adult.test` should be skipped since it is irrelevent to our dataset.\n- In this dataset missing values are marked as ` ?`. We would like to remove all missing values.\n\n::: {#441eb36f .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\ntrain_df = pd.read_csv(\"assests/datasets/adult/adult.data\", na_values=\" ?\", header=None).dropna()\ntest_df = pd.read_csv(\"assests/datasets/adult/adult.test\", na_values=\" ?\", skiprows=[0], header=None).dropna()\n\ncolnames = [\n    \"age\",\n    \"workclass\",\n    \"fnlwgt\",\n    \"education\",\n    \"education-num\",\n    \"marital-status\",\n    \"occupation\",\n    \"relationship\",\n    \"race\",\n    \"sex\",\n    \"capital-gain\",\n    \"capital-loss\",\n    \"hours-per-week\",\n    \"native-country\",\n    \"income\",\n]\n\ntrain_df.columns = colnames\ntest_df.columns = colnames\n```\n:::\n\n\n## Preprocessing\nThere are 15 columns. The last column is `income` which has two string values `' >50K'` and `' <=50K'`. The rest variables are descibed below.\n\n- `age`: continuous\n- `workclass`: 8 categories\n- `fnlwgt`: continuous\n- `education`: 16 categories\n- `education-num`: continuous\n- `marital-status`: 7 categories\n- `occupation`: 14 categories\n- `relationship`: 6 categories\n- `race`: 5 categories\n- `sex`: 2 categories\n- `capital-gain`: continuous\n- `capital-loss`: continuous\n- `hours-per-week`: continuous\n- `native-country`: 41 categories\n\nSo for features we need to normalize numerical columns and apply one-hot encoding for categorical columns. For labels, we need to convert `' >50K'` and `' <=50K'` into `1` and `0`.\n\n### Split the label column\n\n::: {#b0ac0d57 .cell execution_count=2}\n``` {.python .cell-code}\nX_train = train_df.drop(\"income\", axis=1)\ny_train = (train_df[\"income\"] == \" >50K\").astype(int)\nX_test = test_df.drop(\"income\", axis=1) \ny_test = (test_df[\"income\"] == \" >50K\").astype(int)\n```\n:::\n\n\n### Normalize\n\n::: {#95b7af5a .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import MinMaxScaler\n\nnumeric_features = [\n    \"age\",\n    \"fnlwgt\",\n    \"education-num\",\n    \"capital-gain\",\n    \"capital-loss\",\n    \"hours-per-week\",\n]\nmm = MinMaxScaler()\n\nnumeric_X_train = mm.fit_transform(X_train[numeric_features])\nnumeric_X_test = mm.transform(X_test[numeric_features])\n```\n:::\n\n\n### One-hot encoding\n\n::: {#9f6fcd8f .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_features = [\n    \"workclass\",\n    \"education\",\n    \"marital-status\",\n    \"occupation\",\n    \"relationship\",\n    \"race\",\n    \"sex\",\n    \"native-country\",\n]\nohe = OneHotEncoder(handle_unknown=\"ignore\")\n\ncategorical_X_train = ohe.fit_transform(X_train[categorical_features])\ncategorical_X_test = ohe.transform(X_test[categorical_features])\n```\n:::\n\n\nThe onehot decoding of the categorical columns is relative large, and is therefore stored as a sparse matrix (which is an object different from numpy array). So we need to convert it back to numpy array.\n\n::: {#990436f4 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\n\ncategorical_X_train = categorical_X_train.toarray()\ncategorical_X_test = categorical_X_test.toarray()\n```\n:::\n\n\n### Concatenate\n\n::: {#6e5e0ee0 .cell execution_count=6}\n``` {.python .cell-code}\nX_train_norm = np.concatenate([numeric_X_train, categorical_X_train], axis=1)\nX_test_norm = np.concatenate([numeric_X_test, categorical_X_test], axis=1)\n```\n:::\n\n\n### Composer\n\nThe best pratice for the above operations is to write a composer.\n\n::: {#ff6ec886 .cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), numeric_features),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n    ]\n)\n\nX_train_norm = preprocess.fit_transform(X_train)\nX_test_norm = preprocess.transform(X_test)\n```\n:::\n\n\nThe results here are also sparse matrix. So we need to convert it back to dense array.\n\n::: {#71c8a7c7 .cell execution_count=8}\n``` {.python .cell-code}\nX_train_norm = X_train_norm.toarray()\nX_test_norm = X_test_norm.toarray()\n```\n:::\n\n\n## `Dataset` class\n\nWe build a `Dataset` class in order to use it for a nerual network.\n\n::: {#cea6d871 .cell execution_count=9}\n``` {.python .cell-code}\nimport torch\nfrom torch.utils.data import Dataset\n\nclass AdultDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long).reshape(-1, 1)\n\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n\ntrain_ds = AdultDataset(X_train_norm, y_train)\ntest_ds = AdultDataset(X_test_norm, y_test)\n```\n:::\n\n\n",
    "supporting": [
      "adult_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}